{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 1 — INGESTION"
      ],
      "metadata": {
        "id": "A-RugPTH9zLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "SOiK5F7V-Q22",
        "outputId": "b09b2cd0-3318-4b23-d40a-d50ccecf09e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0495b53a-60b3-49df-a903-3ec20eb45e34\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0495b53a-60b3-49df-a903-3ec20eb45e34\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving traffic_data_large.csv to traffic_data_large.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5ISk9goP9ifb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SmartCityTraffic\").getOrCreate()\n",
        "\n",
        "df_raw = spark.read.csv(\"traffic_data_large.csv\", header=True, inferSchema=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_raw.printSchema()\n",
        "print(\"Total records:\", df_raw.count())\n",
        "df_raw.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEgXXIOl_vY_",
        "outputId": "c5af414b-cdbc-4e5e-ae47-efb87124693e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- sensor_id: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- vehicle_count: string (nullable = true)\n",
            " |-- avg_speed: string (nullable = true)\n",
            " |-- temperature: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "Total records: 500000\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "|sensor_id|location |road_name      |vehicle_count|avg_speed|temperature|timestamp          |status  |\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "|S105     |Chennai  |OMR            |invalid      |NULL     |39         |12/01/2026 06:00:00|INACTIVE|\n",
            "|S113     |Chennai  |Mount Road     |103          |73.5     |36         |2026-01-12 06:00:05|ACTIVE  |\n",
            "|S228     |Delhi    |Janpath        |16           |20.0     |35         |2026-01-12 06:00:10|ACTIVE  |\n",
            "|S160     |Bangalore|MG Road        |27           |27.1     |32         |2026-01-12 06:00:15|ACTIVE  |\n",
            "|S252     |Mumbai   |Western Express|115          |59.3     |39         |2026-01-12 06:00:20|ACTIVE  |\n",
            "|S134     |Kolkata  |EM Bypass      |13           |23.6     |29         |2026-01-12 06:00:25|ACTIVE  |\n",
            "|S246     |Delhi    |Janpath        |81           |71.1     |33         |2026-01-12 06:00:30|ACTIVE  |\n",
            "|S154     |Delhi    |Janpath        |40           |36.3     |32         |2026-01-12 06:00:35|ACTIVE  |\n",
            "|S227     |Delhi    |Ring Road      |37           |49.3     |39         |2026-01-12 06:00:40|ACTIVE  |\n",
            "|S221     |Chennai  |GST Road       |89           |79.3     |36         |2026-01-12 06:00:45|ACTIVE  |\n",
            "+---------+---------+---------------+-------------+---------+-----------+-------------------+--------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Identify data quality issues\n",
        "\n",
        "\n",
        "You will observe:\n",
        "\n",
        "* vehicle_count contains invalid\n",
        "* avg_speed has empty strings\n",
        "* timestamps appear in yyyy-MM-dd, dd/MM/yyyy, and yyyy/MM/dd\n",
        "* INACTIVE sensors still send data\n",
        "* temperature exists but is irrelevant for traffic analytics"
      ],
      "metadata": {
        "id": "xDr_mohv_0oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 2 — CLEANING"
      ],
      "metadata": {
        "id": "VlhJRJNQAEJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import trim, col\n",
        "\n",
        "for c in df_raw.columns:\n",
        "    df_raw = df_raw.withColumn(c, trim(col(c)))\n"
      ],
      "metadata": {
        "id": "veaR7XqtAHNs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import when, regexp_replace\n",
        "\n",
        "df = df_raw.withColumn(\n",
        "    \"vehicle_count_clean\",\n",
        "    when(col(\"vehicle_count\").rlike(\"^[0-9]+$\"), col(\"vehicle_count\")).otherwise(None)\n",
        ").withColumn(\"vehicle_count_int\", col(\"vehicle_count_clean\").cast(\"int\"))\n"
      ],
      "metadata": {
        "id": "Gr7oL8SwALjE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df.withColumn(\n",
        "    \"avg_speed_clean\",\n",
        "    when(col(\"avg_speed\").rlike(\"^[0-9.]+$\"), col(\"avg_speed\")).otherwise(None)\n",
        ").withColumn(\"avg_speed_double\", col(\"avg_speed_clean\").cast(\"double\"))\n"
      ],
      "metadata": {
        "id": "ltmB8u9tAPUY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce, try_to_timestamp, col, lit\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"event_time\",\n",
        "    coalesce(\n",
        "        try_to_timestamp(col(\"timestamp\"), lit(\"yyyy-MM-dd HH:mm:ss\")),\n",
        "        try_to_timestamp(col(\"timestamp\"), lit(\"dd/MM/yyyy HH:mm:ss\")),\n",
        "        try_to_timestamp(col(\"timestamp\"), lit(\"yyyy/MM/dd HH:mm:ss\"))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "JWY6wNqRAR9i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep original timestamp for audit\n",
        "\n",
        "\n",
        "Already preserved as timestamp."
      ],
      "metadata": {
        "id": "mdUtANWdAa-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 3 — VALIDATION"
      ],
      "metadata": {
        "id": "672MCh1JAhT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "invalid_vehicle = df.filter(col(\"vehicle_count_int\").isNull()).count()\n",
        "print(\"Invalid vehicle_count:\", invalid_vehicle)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnYXnjGLAkQ2",
        "outputId": "74ab6a86-aea9-4dc8-9955-9b959249220f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid vehicle_count: 49873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "invalid_ts = df.filter(col(\"event_time\").isNull()).count()\n",
        "print(\"Invalid timestamps:\", invalid_ts)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUwjHqI6AnCW",
        "outputId": "8291d640-366b-414f-a131-a08b8aec9e86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid timestamps: 4853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_active = df.filter(col(\"status\") == \"ACTIVE\")"
      ],
      "metadata": {
        "id": "tkbeok_VAtmB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After ACTIVE filter:\", df_active.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_njRikbAwn3",
        "outputId": "931009ff-d070-4603-b922-79be15ecfbc0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After ACTIVE filter: 475000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 4 — TRAFFIC METRICS"
      ],
      "metadata": {
        "id": "QC9plo0KAy1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_speed_loc = df_active.groupBy(\"location\").avg(\"avg_speed_double\")"
      ],
      "metadata": {
        "id": "ITf8JTdqA2YW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_per_road = df_active.groupBy(\"road_name\").sum(\"vehicle_count_int\")"
      ],
      "metadata": {
        "id": "6GcgSK3JA6mE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import max as Fmax\n",
        "\n",
        "peak_time_loc = df_active.groupBy(\"location\") \\\n",
        "    .agg(Fmax(\"vehicle_count_int\").alias(\"peak_vehicles\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "yvU4rHxUA-nM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "congestion = df_active.groupBy(\"road_name\").avg(\"avg_speed_double\") \\\n",
        "    .orderBy(\"avg(avg_speed_double)\")\n"
      ],
      "metadata": {
        "id": "da6VcpDqBElC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 5 — WINDOW FUNCTIONS"
      ],
      "metadata": {
        "id": "xI1FiHmuBG-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "road_speed = df_active.groupBy(\"road_name\").avg(\"avg_speed_double\")\n",
        "\n",
        "w = Window.orderBy(col(\"avg(avg_speed_double)\").asc())\n",
        "\n",
        "road_rank = road_speed.withColumn(\"congestion_rank\", rank().over(w))\n"
      ],
      "metadata": {
        "id": "-Xw7M_Q_BKLe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "road_loc_df = df_active.groupBy(\"location\",\"road_name\") \\\n",
        "    .sum(\"vehicle_count_int\")\n",
        "\n",
        "w_loc = Window.partitionBy(\"location\").orderBy(col(\"sum(vehicle_count_int)\").desc())\n",
        "\n",
        "road_loc_rank = road_loc_df.withColumn(\"road_rank\", rank().over(w_loc))\n"
      ],
      "metadata": {
        "id": "9APGEUTIC9Kf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top3_congested = road_loc_rank.filter(col(\"road_rank\") <= 3)"
      ],
      "metadata": {
        "id": "XWnYVAcjDAbr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 6 — ANOMALY DETECTION"
      ],
      "metadata": {
        "id": "V85AsrVnDCLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lag\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, col\n",
        "\n",
        "w_event = Window.partitionBy(\"sensor_id\").orderBy(\"event_time\")\n",
        "\n",
        "df_anom = df_active.withColumn(\"prev_speed\", lag(\"avg_speed_double\").over(w_event)) \\\n",
        "                   .withColumn(\"speed_drop\", col(\"prev_speed\") - col(\"avg_speed_double\"))"
      ],
      "metadata": {
        "id": "YI-v3l4UDEVH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudden_drop = df_anom.filter(col(\"speed_drop\") > 10)"
      ],
      "metadata": {
        "id": "c3Lvse6lDUWJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_anom = df_anom.withColumn(\"prev_count\", lag(\"vehicle_count_int\").over(w_event)) \\\n",
        "                 .withColumn(\"count_spike\", col(\"vehicle_count_int\") - col(\"prev_count\"))\n",
        "\n",
        "sudden_spike = df_anom.filter(col(\"count_spike\") > 20)\n",
        "\n"
      ],
      "metadata": {
        "id": "d--EeYl-DXOX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 7 — PERFORMANCE ENGINEERING"
      ],
      "metadata": {
        "id": "1kpaM6mFDaKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_active.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luqrRYQkDcPz",
        "outputId": "d568cdc1-77dd-45d0-f714-cac043dccb55"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "congestion.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqlaEBlTDfGC",
        "outputId": "c7b7748b-e6f2-4b5e-fd60-2cea6fbef071"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Sort ['avg(avg_speed_double) ASC NULLS FIRST], true\n",
            "+- Aggregate [road_name#73], [road_name#73, avg(avg_speed_double#82) AS avg(avg_speed_double)#198]\n",
            "   +- Filter (status#78 = ACTIVE)\n",
            "      +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, avg_speed_double#82, coalesce(try_to_timestamp(timestamp#77, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(dd/MM/yyyy HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(yyyy/MM/dd HH:mm:ss), TimestampType, Some(Etc/UTC), false)) AS event_time#84]\n",
            "         +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, cast(avg_speed_clean#81 as double) AS avg_speed_double#82]\n",
            "            +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN avg_speed#75 ELSE cast(null as string) END AS avg_speed_clean#81]\n",
            "               +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, cast(vehicle_count_clean#79 as int) AS vehicle_count_int#80]\n",
            "                  +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, CASE WHEN RLIKE(vehicle_count#74, ^[0-9]+$) THEN vehicle_count#74 ELSE cast(null as string) END AS vehicle_count_clean#79]\n",
            "                     +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, trim(status#24, None) AS status#78]\n",
            "                        +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, trim(timestamp#23, None) AS timestamp#77, status#24]\n",
            "                           +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, trim(temperature#22, None) AS temperature#76, timestamp#23, status#24]\n",
            "                              +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, trim(avg_speed#21, None) AS avg_speed#75, temperature#22, timestamp#23, status#24]\n",
            "                                 +- Project [sensor_id#71, location#72, road_name#73, trim(vehicle_count#20, None) AS vehicle_count#74, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                    +- Project [sensor_id#71, location#72, trim(road_name#19, None) AS road_name#73, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                       +- Project [sensor_id#71, trim(location#18, None) AS location#72, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                          +- Project [trim(sensor_id#17, None) AS sensor_id#71, location#18, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                             +- Relation [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "road_name: string, avg(avg_speed_double): double\n",
            "Sort [avg(avg_speed_double)#198 ASC NULLS FIRST], true\n",
            "+- Aggregate [road_name#73], [road_name#73, avg(avg_speed_double#82) AS avg(avg_speed_double)#198]\n",
            "   +- Filter (status#78 = ACTIVE)\n",
            "      +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, avg_speed_double#82, coalesce(try_to_timestamp(timestamp#77, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(dd/MM/yyyy HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(yyyy/MM/dd HH:mm:ss), TimestampType, Some(Etc/UTC), false)) AS event_time#84]\n",
            "         +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, cast(avg_speed_clean#81 as double) AS avg_speed_double#82]\n",
            "            +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN avg_speed#75 ELSE cast(null as string) END AS avg_speed_clean#81]\n",
            "               +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, cast(vehicle_count_clean#79 as int) AS vehicle_count_int#80]\n",
            "                  +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, CASE WHEN RLIKE(vehicle_count#74, ^[0-9]+$) THEN vehicle_count#74 ELSE cast(null as string) END AS vehicle_count_clean#79]\n",
            "                     +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, trim(status#24, None) AS status#78]\n",
            "                        +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, trim(timestamp#23, None) AS timestamp#77, status#24]\n",
            "                           +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, trim(temperature#22, None) AS temperature#76, timestamp#23, status#24]\n",
            "                              +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, trim(avg_speed#21, None) AS avg_speed#75, temperature#22, timestamp#23, status#24]\n",
            "                                 +- Project [sensor_id#71, location#72, road_name#73, trim(vehicle_count#20, None) AS vehicle_count#74, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                    +- Project [sensor_id#71, location#72, trim(road_name#19, None) AS road_name#73, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                       +- Project [sensor_id#71, trim(location#18, None) AS location#72, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                          +- Project [trim(sensor_id#17, None) AS sensor_id#71, location#18, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                             +- Relation [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [avg(avg_speed_double)#198 ASC NULLS FIRST], true\n",
            "+- Aggregate [road_name#73], [road_name#73, avg(avg_speed_double#82) AS avg(avg_speed_double)#198]\n",
            "   +- Project [road_name#73, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN cast(avg_speed#75 as double) END AS avg_speed_double#82]\n",
            "      +- Project [trim(road_name#19, None) AS road_name#73, trim(avg_speed#21, None) AS avg_speed#75]\n",
            "         +- Filter (trim(status#24, None) = ACTIVE)\n",
            "            +- Relation [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [avg(avg_speed_double)#198 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(avg(avg_speed_double)#198 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=249]\n",
            "      +- HashAggregate(keys=[road_name#73], functions=[avg(avg_speed_double#82)], output=[road_name#73, avg(avg_speed_double)#198])\n",
            "         +- Exchange hashpartitioning(road_name#73, 200), ENSURE_REQUIREMENTS, [plan_id=246]\n",
            "            +- HashAggregate(keys=[road_name#73], functions=[partial_avg(avg_speed_double#82)], output=[road_name#73, sum#261, count#262L])\n",
            "               +- Project [road_name#73, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN cast(avg_speed#75 as double) END AS avg_speed_double#82]\n",
            "                  +- Project [trim(road_name#19, None) AS road_name#73, trim(avg_speed#21, None) AS avg_speed#75]\n",
            "                     +- Filter (trim(status#24, None) = ACTIVE)\n",
            "                        +- FileScan csv [road_name#19,avg_speed#21,status#24] Batched: false, DataFilters: [(trim(status#24, None) = ACTIVE)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<road_name:string,avg_speed:string,status:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_part = df_active.repartition(\"location\")"
      ],
      "metadata": {
        "id": "ttWnAC9dDjB0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_active.cache()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4YF0lEyDlka",
        "outputId": "b70909ea-5de5-4ff0-9788-bf008427fc54"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[sensor_id: string, location: string, road_name: string, vehicle_count: string, avg_speed: string, temperature: string, timestamp: string, status: string, vehicle_count_clean: string, vehicle_count_int: int, avg_speed_clean: string, avg_speed_double: double, event_time: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_part.groupBy(\"location\").avg(\"avg_speed_double\").explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9-VgxsmDoXQ",
        "outputId": "ee81c1cf-e0c0-4eb8-dc27-018e7d0f8b34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['location], ['location, unresolvedalias('avg(avg_speed_double#82))]\n",
            "+- RepartitionByExpression [location#72]\n",
            "   +- Filter (status#78 = ACTIVE)\n",
            "      +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, avg_speed_double#82, coalesce(try_to_timestamp(timestamp#77, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(dd/MM/yyyy HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(yyyy/MM/dd HH:mm:ss), TimestampType, Some(Etc/UTC), false)) AS event_time#84]\n",
            "         +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, cast(avg_speed_clean#81 as double) AS avg_speed_double#82]\n",
            "            +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN avg_speed#75 ELSE cast(null as string) END AS avg_speed_clean#81]\n",
            "               +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, cast(vehicle_count_clean#79 as int) AS vehicle_count_int#80]\n",
            "                  +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, CASE WHEN RLIKE(vehicle_count#74, ^[0-9]+$) THEN vehicle_count#74 ELSE cast(null as string) END AS vehicle_count_clean#79]\n",
            "                     +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, trim(status#24, None) AS status#78]\n",
            "                        +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, trim(timestamp#23, None) AS timestamp#77, status#24]\n",
            "                           +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, trim(temperature#22, None) AS temperature#76, timestamp#23, status#24]\n",
            "                              +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, trim(avg_speed#21, None) AS avg_speed#75, temperature#22, timestamp#23, status#24]\n",
            "                                 +- Project [sensor_id#71, location#72, road_name#73, trim(vehicle_count#20, None) AS vehicle_count#74, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                    +- Project [sensor_id#71, location#72, trim(road_name#19, None) AS road_name#73, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                       +- Project [sensor_id#71, trim(location#18, None) AS location#72, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                          +- Project [trim(sensor_id#17, None) AS sensor_id#71, location#18, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                             +- Relation [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "location: string, avg(avg_speed_double): double\n",
            "Aggregate [location#72], [location#72, avg(avg_speed_double#82) AS avg(avg_speed_double)#342]\n",
            "+- RepartitionByExpression [location#72]\n",
            "   +- Filter (status#78 = ACTIVE)\n",
            "      +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, avg_speed_double#82, coalesce(try_to_timestamp(timestamp#77, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(dd/MM/yyyy HH:mm:ss), TimestampType, Some(Etc/UTC), false), try_to_timestamp(timestamp#77, Some(yyyy/MM/dd HH:mm:ss), TimestampType, Some(Etc/UTC), false)) AS event_time#84]\n",
            "         +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, cast(avg_speed_clean#81 as double) AS avg_speed_double#82]\n",
            "            +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN avg_speed#75 ELSE cast(null as string) END AS avg_speed_clean#81]\n",
            "               +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, cast(vehicle_count_clean#79 as int) AS vehicle_count_int#80]\n",
            "                  +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, CASE WHEN RLIKE(vehicle_count#74, ^[0-9]+$) THEN vehicle_count#74 ELSE cast(null as string) END AS vehicle_count_clean#79]\n",
            "                     +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, trim(status#24, None) AS status#78]\n",
            "                        +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, trim(timestamp#23, None) AS timestamp#77, status#24]\n",
            "                           +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, trim(temperature#22, None) AS temperature#76, timestamp#23, status#24]\n",
            "                              +- Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, trim(avg_speed#21, None) AS avg_speed#75, temperature#22, timestamp#23, status#24]\n",
            "                                 +- Project [sensor_id#71, location#72, road_name#73, trim(vehicle_count#20, None) AS vehicle_count#74, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                    +- Project [sensor_id#71, location#72, trim(road_name#19, None) AS road_name#73, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                       +- Project [sensor_id#71, trim(location#18, None) AS location#72, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                          +- Project [trim(sensor_id#17, None) AS sensor_id#71, location#18, road_name#19, vehicle_count#20, avg_speed#21, temperature#22, timestamp#23, status#24]\n",
            "                                             +- Relation [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [location#72], [location#72, avg(avg_speed_double#82) AS avg(avg_speed_double)#342]\n",
            "+- RepartitionByExpression [location#72]\n",
            "   +- Project [location#72, avg_speed_double#82]\n",
            "      +- InMemoryRelation [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, avg_speed_double#82, event_time#84], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "            +- *(1) Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, cast(avg_speed_clean#81 as double) AS avg_speed_double#82, coalesce(gettimestamp(timestamp#77, yyyy-MM-dd HH:mm:ss, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(timestamp#77, dd/MM/yyyy HH:mm:ss, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(timestamp#77, yyyy/MM/dd HH:mm:ss, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS event_time#84]\n",
            "               +- *(1) Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, cast(vehicle_count_clean#79 as int) AS vehicle_count_int#80, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN avg_speed#75 END AS avg_speed_clean#81]\n",
            "                  +- *(1) Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, CASE WHEN RLIKE(vehicle_count#74, ^[0-9]+$) THEN vehicle_count#74 END AS vehicle_count_clean#79]\n",
            "                     +- *(1) Project [trim(sensor_id#17, None) AS sensor_id#71, trim(location#18, None) AS location#72, trim(road_name#19, None) AS road_name#73, trim(vehicle_count#20, None) AS vehicle_count#74, trim(avg_speed#21, None) AS avg_speed#75, trim(temperature#22, None) AS temperature#76, trim(timestamp#23, None) AS timestamp#77, trim(status#24, None) AS status#78]\n",
            "                        +- *(1) Filter (trim(status#24, None) = ACTIVE)\n",
            "                           +- FileScan csv [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] Batched: false, DataFilters: [(trim(status#24, None) = ACTIVE)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,te...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[location#72], functions=[avg(avg_speed_double#82)], output=[location#72, avg(avg_speed_double)#342])\n",
            "   +- HashAggregate(keys=[location#72], functions=[partial_avg(avg_speed_double#82)], output=[location#72, sum#540, count#541L])\n",
            "      +- Exchange hashpartitioning(location#72, 200), REPARTITION_BY_COL, [plan_id=288]\n",
            "         +- InMemoryTableScan [location#72, avg_speed_double#82]\n",
            "               +- InMemoryRelation [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, avg_speed_double#82, event_time#84], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                     +- *(1) Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, vehicle_count_int#80, avg_speed_clean#81, cast(avg_speed_clean#81 as double) AS avg_speed_double#82, coalesce(gettimestamp(timestamp#77, yyyy-MM-dd HH:mm:ss, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(timestamp#77, dd/MM/yyyy HH:mm:ss, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(timestamp#77, yyyy/MM/dd HH:mm:ss, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS event_time#84]\n",
            "                        +- *(1) Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, vehicle_count_clean#79, cast(vehicle_count_clean#79 as int) AS vehicle_count_int#80, CASE WHEN RLIKE(avg_speed#75, ^[0-9.]+$) THEN avg_speed#75 END AS avg_speed_clean#81]\n",
            "                           +- *(1) Project [sensor_id#71, location#72, road_name#73, vehicle_count#74, avg_speed#75, temperature#76, timestamp#77, status#78, CASE WHEN RLIKE(vehicle_count#74, ^[0-9]+$) THEN vehicle_count#74 END AS vehicle_count_clean#79]\n",
            "                              +- *(1) Project [trim(sensor_id#17, None) AS sensor_id#71, trim(location#18, None) AS location#72, trim(road_name#19, None) AS road_name#73, trim(vehicle_count#20, None) AS vehicle_count#74, trim(avg_speed#21, None) AS avg_speed#75, trim(temperature#22, None) AS temperature#76, trim(timestamp#23, None) AS timestamp#77, trim(status#24, None) AS status#78]\n",
            "                                 +- *(1) Filter (trim(status#24, None) = ACTIVE)\n",
            "                                    +- FileScan csv [sensor_id#17,location#18,road_name#19,vehicle_count#20,avg_speed#21,temperature#22,timestamp#23,status#24] Batched: false, DataFilters: [(trim(status#24, None) = ACTIVE)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/traffic_data_large.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sensor_id:string,location:string,road_name:string,vehicle_count:string,avg_speed:string,te...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 8 — RDD OPERATIONS"
      ],
      "metadata": {
        "id": "QzH8wqLtDqih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df_active.rdd"
      ],
      "metadata": {
        "id": "vTixDDzgDtLh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_vehicle = rdd.map(lambda r: r[\"vehicle_count_int\"] or 0).reduce(lambda a,b: a+b)"
      ],
      "metadata": {
        "id": "4erdqiJ3DwYU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loc_counts = rdd.map(lambda r: (r[\"location\"], 1)).reduceByKey(lambda a,b: a+b)"
      ],
      "metadata": {
        "id": "8ehFZMhvD4Nm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why DataFrames are better?\n",
        "\n",
        "* Catalyst optimizer = smarter query plans\n",
        "* Tungsten execution = faster memory handling\n",
        "* Column pruning, predicate pushdown\n",
        "* Much less code\n",
        "* RDDs lack schema and optimizations → slower and error-prone"
      ],
      "metadata": {
        "id": "7ZDbv0hPD8ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 9 — SORTING & SET OPS"
      ],
      "metadata": {
        "id": "vtCSDdqwEDQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "high_congestion = road_speed.orderBy(col(\"avg(avg_speed_double)\").asc())\n",
        "\n"
      ],
      "metadata": {
        "id": "On2RMgAfEFbm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "low_speed_set = df_active.filter(col(\"avg_speed_double\") < 25).select(\"road_name\").distinct()\n",
        "high_traffic_set = df_active.filter(col(\"vehicle_count_int\") > 60).select(\"road_name\").distinct()\n"
      ],
      "metadata": {
        "id": "t6X9cEmUEI6h"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "both = low_speed_set.join(high_traffic_set, \"road_name\", \"inner\")\n",
        "only_low_speed = low_speed_set.join(high_traffic_set, \"road_name\", \"left_anti\")\n",
        "only_high_traffic = high_traffic_set.join(low_speed_set, \"road_name\", \"left_anti\")\n"
      ],
      "metadata": {
        "id": "W27N8aqvEK3s"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 10 — STORAGE"
      ],
      "metadata": {
        "id": "kgms8g60EMTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_active.write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"location\") \\\n",
        "    .parquet(\"traffic_clean_parquet\")\n"
      ],
      "metadata": {
        "id": "W1cXrvFYEOx5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "congestion.write.mode(\"overwrite\").orc(\"congestion_orc\")"
      ],
      "metadata": {
        "id": "GaSKrwmDEVc4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p = spark.read.parquet(\"traffic_clean_parquet\")\n",
        "o = spark.read.orc(\"congestion_orc\")\n",
        "\n",
        "p.printSchema()\n",
        "o.printSchema()\n",
        "\n",
        "print(p.count(), o.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5e3FOMREYyR",
        "outputId": "40a84d01-8702-404a-8ff9-4adc0fbbe8b1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- sensor_id: string (nullable = true)\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- vehicle_count: string (nullable = true)\n",
            " |-- avg_speed: string (nullable = true)\n",
            " |-- temperature: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- vehicle_count_clean: string (nullable = true)\n",
            " |-- vehicle_count_int: integer (nullable = true)\n",
            " |-- avg_speed_clean: string (nullable = true)\n",
            " |-- avg_speed_double: double (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- road_name: string (nullable = true)\n",
            " |-- avg(avg_speed_double): double (nullable = true)\n",
            "\n",
            "475000 21\n"
          ]
        }
      ]
    }
  ]
}