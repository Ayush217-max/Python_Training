{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Open a file picker to upload\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ykePqPMMLukj",
        "outputId": "a2aada87-dbee-48a4-bd00-d91eb352b0fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8051aae4-6f4b-4f4d-8c19-a060f8e83601\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8051aae4-6f4b-4f4d-8c19-a060f8e83601\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving orders.csv to orders.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bwZPYVV7K0Bw"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OrdersPipeline\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df_raw = spark.read.csv(\"orders.csv\", header=True, inferSchema=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO47M2gxMs7t",
        "outputId": "963c4c70-aa32-42a6-f843-eeef86f51053"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_count = df_raw.count()\n",
        "print(\"Total Records =\", raw_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQHYqTYZMu7W",
        "outputId": "a3003458-2e3a-420e-fdda-95d80f59f9af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Records = 300000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Wj5ASIMypO",
        "outputId": "c5c17ece-1b29-490e-f0ea-20c183dd1d65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|order_id   |customer_id|city       |category   |product    |amount |order_date|status   |\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|C000000    | hyderabad | grocery   |Oil        |invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|C000001    |Pune       |Grocery    |Sugar      |35430  |2024-01-02|Completed|\n",
            "|ORD00000002|C000002    |Pune       |Electronics|Mobile     |65358  |2024-01-03|Completed|\n",
            "|ORD00000003|C000003    |Bangalore  |Electronics|Laptop     |5558   |2024-01-04|Completed|\n",
            "|ORD00000004|C000004    |Pune       |Home       |AirPurifier|33659  |2024-01-05|Completed|\n",
            "|ORD00000005|C000005    |Delhi      |Fashion    |Jeans      |8521   |2024-01-06|Completed|\n",
            "|ORD00000006|C000006    |Delhi      |Grocery    |Sugar      |42383  |2024-01-07|Completed|\n",
            "|ORD00000007|C000007    |Pune       |Grocery    |Rice       |45362  |2024-01-08|Completed|\n",
            "|ORD00000008|C000008    |Bangalore  |Fashion    |Jeans      |10563  |2024-01-09|Completed|\n",
            "|ORD00000009|C000009    |Kolkata    |Electronics|Laptop     |63715  |2024-01-10|Completed|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 2 — DATA CLEANING"
      ],
      "metadata": {
        "id": "dlFuPYLAM3yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import trim, col\n",
        "\n",
        "df = df_raw.withColumn(\"city\", trim(col(\"city\"))) \\\n",
        "           .withColumn(\"category\", trim(col(\"category\"))) \\\n",
        "           .withColumn(\"product\", trim(col(\"product\")))\n"
      ],
      "metadata": {
        "id": "HMigCBdMM59X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import initcap\n",
        "\n",
        "df = df.withColumn(\"city_clean\", initcap(col(\"city\"))) \\\n",
        "       .withColumn(\"category_clean\", initcap(col(\"category\"))) \\\n",
        "       .withColumn(\"product_clean\", initcap(col(\"product\")))\n"
      ],
      "metadata": {
        "id": "qCKokbe7M9U2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import regexp_replace, when\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"amount_clean\",\n",
        "    regexp_replace(\"amount\", \",\", \"\")\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"amount_clean\",\n",
        "    when(col(\"amount_clean\").rlike(\"^[0-9]+$\"), col(\"amount_clean\")).otherwise(None)\n",
        ")\n",
        "\n",
        "df = df.withColumn(\"amount_int\", col(\"amount_clean\").cast(\"int\"))\n"
      ],
      "metadata": {
        "id": "rp2LIk_7NBcD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce, to_date, try_to_timestamp, col, lit\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"order_date_clean\",\n",
        "    coalesce(\n",
        "        try_to_timestamp(col(\"order_date\"), lit(\"yyyy-MM-dd\")),\n",
        "        try_to_timestamp(col(\"order_date\"), lit(\"dd/MM/yyyy\")),\n",
        "        try_to_timestamp(col(\"order_date\"), lit(\"yyyy/MM/dd\"))\n",
        "    ).cast(\"date\")\n",
        ")"
      ],
      "metadata": {
        "id": "8wgCXx8pNECW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 3 — DATA VALIDATION"
      ],
      "metadata": {
        "id": "1QEtkZSzNHuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_amounts = df.filter(col(\"amount_int\").isNull()).count()"
      ],
      "metadata": {
        "id": "798hQ9LwNKI-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# The previous line 'df = df.withColumn(\"parsed_date\", to_timestamp(\"date_column\", \"dd/MM/yyyy\"))' was likely a leftover or a test and not intended to be part of the final flow here,\n",
        "# as it refers to a non-existent 'date_column' and would override the cleaned date column if it were named 'order_date_clean'.\n",
        "# I'm removing it to avoid introducing new issues and to ensure we're using the 'order_date_clean' column generated in the previous step.\n",
        "# df = df.withColumn(\"parsed_date\", to_timestamp(\"date_column\", \"dd/MM/yyyy\"))\n",
        "\n",
        "invalid_dates = df.filter(col(\"order_date_clean\").isNull()).count()"
      ],
      "metadata": {
        "id": "rNhB55aGNNmE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "df.groupBy(\"order_id\").agg(count(\"*\").alias(\"cnt\")).filter(\"cnt > 1\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edctE5abUvUg",
        "outputId": "ffde801a-ce28-4733-e36f-cbadc93b79b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+\n",
            "|order_id|cnt|\n",
            "+--------+---+\n",
            "+--------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropDuplicates([\"order_id\"])"
      ],
      "metadata": {
        "id": "LakEPbbXUy-Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_completed = df.filter(col(\"status\") == \"Completed\")"
      ],
      "metadata": {
        "id": "Ocvc8PwlU2AH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"After cleaning:\", df.count())\n",
        "print(\"After filtering completed:\", df_completed.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beiEmteoU5WC",
        "outputId": "62bca293-400f-42b7-fb9b-913cf23ba46e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After cleaning: 300000\n",
            "After filtering completed: 285000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 4 — PERFORMANCE ENGINEERING"
      ],
      "metadata": {
        "id": "x7tjc4LOU-8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(df.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hVVgLAhU_sS",
        "outputId": "8522ee82-25af-4bdf-96e8-2f2af3600fcb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_rev = df.groupBy(\"city_clean\").sum(\"amount_int\")"
      ],
      "metadata": {
        "id": "YFYZxTkiVJtz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_rev.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR4aZu49VMqI",
        "outputId": "e01d7943-cfc0-4661-a702-4263300f69d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city_clean], ['city_clean, unresolvedalias('sum(amount_int#79))]\n",
            "+- Deduplicate [order_id#17]\n",
            "   +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, cast(coalesce(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            "      +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#123) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#124]\n",
            "         +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#122) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#123]\n",
            "            +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#122]\n",
            "               +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#81) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#82]\n",
            "                  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#80) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#81]\n",
            "                     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#80]\n",
            "                        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79]\n",
            "                           +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 ELSE cast(null as string) END AS amount_clean#78]\n",
            "                              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, initcap(product#73) AS product_clean#76]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, initcap(category#72) AS category_clean#75]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#71, category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            "                                             +- Project [order_id#17, customer_id#18, city#71, trim(category#20, None) AS category#72, product#21, amount#22, order_date#23, status#24]\n",
            "                                                +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, category#20, product#21, amount#22, order_date#23, status#24]\n",
            "                                                   +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city_clean: string, sum(amount_int): bigint\n",
            "Aggregate [city_clean#74], [city_clean#74, sum(amount_int#79) AS sum(amount_int)#429L]\n",
            "+- Deduplicate [order_id#17]\n",
            "   +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, cast(coalesce(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            "      +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#123) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#124]\n",
            "         +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#122) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#123]\n",
            "            +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#122]\n",
            "               +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#81) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#82]\n",
            "                  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#80) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#81]\n",
            "                     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#80]\n",
            "                        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79]\n",
            "                           +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 ELSE cast(null as string) END AS amount_clean#78]\n",
            "                              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, initcap(product#73) AS product_clean#76]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, initcap(category#72) AS category_clean#75]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#71, category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            "                                             +- Project [order_id#17, customer_id#18, city#71, trim(category#20, None) AS category#72, product#21, amount#22, order_date#23, status#24]\n",
            "                                                +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, category#20, product#21, amount#22, order_date#23, status#24]\n",
            "                                                   +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city_clean#445], [city_clean#445, sum(amount_int#453) AS sum(amount_int)#429L]\n",
            "+- Aggregate [order_id#17], [first(city_clean#74, false) AS city_clean#445, first(amount_int#79, false) AS amount_int#453]\n",
            "   +- Project [order_id#17, city_clean#74, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN cast(amount_clean#77 as int) END AS amount_int#79]\n",
            "      +- Project [order_id#17, initcap(trim(city#19, None)) AS city_clean#74, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "         +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[city_clean#445], functions=[sum(amount_int#453)], output=[city_clean#445, sum(amount_int)#429L])\n",
            "   +- Exchange hashpartitioning(city_clean#445, 200), ENSURE_REQUIREMENTS, [plan_id=600]\n",
            "      +- HashAggregate(keys=[city_clean#445], functions=[partial_sum(amount_int#453)], output=[city_clean#445, sum#457L])\n",
            "         +- SortAggregate(key=[order_id#17], functions=[first(city_clean#74, false), first(amount_int#79, false)], output=[city_clean#445, amount_int#453])\n",
            "            +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "               +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=595]\n",
            "                  +- SortAggregate(key=[order_id#17], functions=[partial_first(city_clean#74, false), partial_first(amount_int#79, false)], output=[order_id#17, first#462, valueSet#463, first#464, valueSet#465])\n",
            "                     +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                        +- Project [order_id#17, city_clean#74, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN cast(amount_clean#77 as int) END AS amount_int#79]\n",
            "                           +- Project [order_id#17, initcap(trim(city#19, None)) AS city_clean#74, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "                              +- FileScan csv [order_id#17,city#19,amount#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Identify shuffle\n",
        "\n",
        "GroupBy always causes a shuffle because it requires same-key data to be co-located."
      ],
      "metadata": {
        "id": "ZV2in-uqVVaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rep = df.repartition(\"city_clean\")"
      ],
      "metadata": {
        "id": "scuiTVt4VP8I"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rep.groupBy(\"city_clean\").sum(\"amount_int\").explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBd13MSsVe3P",
        "outputId": "058aadda-1f1a-493e-e885-ec8f2257f7ec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city_clean], ['city_clean, unresolvedalias('sum(amount_int#79))]\n",
            "+- RepartitionByExpression [city_clean#74]\n",
            "   +- Deduplicate [order_id#17]\n",
            "      +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, cast(coalesce(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            "         +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#123) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#124]\n",
            "            +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#122) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#123]\n",
            "               +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#122]\n",
            "                  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#81) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#82]\n",
            "                     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#80) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#81]\n",
            "                        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#80]\n",
            "                           +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79]\n",
            "                              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 ELSE cast(null as string) END AS amount_clean#78]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, initcap(product#73) AS product_clean#76]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, initcap(category#72) AS category_clean#75]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74]\n",
            "                                             +- Project [order_id#17, customer_id#18, city#71, category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            "                                                +- Project [order_id#17, customer_id#18, city#71, trim(category#20, None) AS category#72, product#21, amount#22, order_date#23, status#24]\n",
            "                                                   +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, category#20, product#21, amount#22, order_date#23, status#24]\n",
            "                                                      +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city_clean: string, sum(amount_int): bigint\n",
            "Aggregate [city_clean#74], [city_clean#74, sum(amount_int#79) AS sum(amount_int)#481L]\n",
            "+- RepartitionByExpression [city_clean#74]\n",
            "   +- Deduplicate [order_id#17]\n",
            "      +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, cast(coalesce(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            "         +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#123) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#124]\n",
            "            +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#122) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#123]\n",
            "               +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#122]\n",
            "                  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#81) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#82]\n",
            "                     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#80) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#81]\n",
            "                        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#80]\n",
            "                           +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79]\n",
            "                              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 ELSE cast(null as string) END AS amount_clean#78]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "                                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, initcap(product#73) AS product_clean#76]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, initcap(category#72) AS category_clean#75]\n",
            "                                          +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74]\n",
            "                                             +- Project [order_id#17, customer_id#18, city#71, category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            "                                                +- Project [order_id#17, customer_id#18, city#71, trim(category#20, None) AS category#72, product#21, amount#22, order_date#23, status#24]\n",
            "                                                   +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, category#20, product#21, amount#22, order_date#23, status#24]\n",
            "                                                      +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city_clean#497], [city_clean#497, sum(amount_int#505) AS sum(amount_int)#481L]\n",
            "+- RepartitionByExpression [city_clean#497]\n",
            "   +- Aggregate [order_id#17], [first(city_clean#74, false) AS city_clean#497, first(amount_int#79, false) AS amount_int#505]\n",
            "      +- Project [order_id#17, city_clean#74, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN cast(amount_clean#77 as int) END AS amount_int#79]\n",
            "         +- Project [order_id#17, initcap(trim(city#19, None)) AS city_clean#74, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "            +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[city_clean#497], functions=[sum(amount_int#505)], output=[city_clean#497, sum(amount_int)#481L])\n",
            "   +- HashAggregate(keys=[city_clean#497], functions=[partial_sum(amount_int#505)], output=[city_clean#497, sum#509L])\n",
            "      +- Exchange hashpartitioning(city_clean#497, 200), REPARTITION_BY_COL, [plan_id=636]\n",
            "         +- SortAggregate(key=[order_id#17], functions=[first(city_clean#74, false), first(amount_int#79, false)], output=[city_clean#497, amount_int#505])\n",
            "            +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "               +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=633]\n",
            "                  +- SortAggregate(key=[order_id#17], functions=[partial_first(city_clean#74, false), partial_first(amount_int#79, false)], output=[order_id#17, first#514, valueSet#515, first#516, valueSet#517])\n",
            "                     +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                        +- Project [order_id#17, city_clean#74, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN cast(amount_clean#77 as int) END AS amount_int#79]\n",
            "                           +- Project [order_id#17, initcap(trim(city#19, None)) AS city_clean#74, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "                              +- FileScan csv [order_id#17,city#19,amount#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 5 — ANALYTICS"
      ],
      "metadata": {
        "id": "WDOrkgJMVh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rev_city = df.groupBy(\"city_clean\").sum(\"amount_int\")\n"
      ],
      "metadata": {
        "id": "ifOlCVh6Vi0A"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_cat = df.groupBy(\"category_clean\").sum(\"amount_int\")"
      ],
      "metadata": {
        "id": "bRzsHrhlVnUO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_city = df.groupBy(\"city_clean\").avg(\"amount_int\")"
      ],
      "metadata": {
        "id": "JUiGa8hiVptn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "top_products = df.groupBy(\"product_clean\").sum(\"amount_int\")\\\n",
        "                 .orderBy(col(\"sum(amount_int)\").desc())\\\n",
        "                 .limit(10)\n"
      ],
      "metadata": {
        "id": "0M715WjgVsMP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_city.orderBy(col(\"sum(amount_int)\").desc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f16pJAoIVu5k",
        "outputId": "1ca5890a-464d-4a94-9539-9498e6ff71f4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[city_clean: string, sum(amount_int): bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 6 — WINDOW FUNCTIONS"
      ],
      "metadata": {
        "id": "xl-3wbFIV0QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, dense_rank\n"
      ],
      "metadata": {
        "id": "rir27tdMV3o-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w = Window.orderBy(col(\"sum(amount_int)\").desc())\n",
        "city_rank = rev_city.withColumn(\"rank\", rank().over(w))\n"
      ],
      "metadata": {
        "id": "BVznQy4fV7By"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wcat = Window.partitionBy(\"category_clean\").orderBy(col(\"sum(amount_int)\").desc())\n",
        "prod_rank = df.groupBy(\"category_clean\",\"product_clean\").sum(\"amount_int\") \\\n",
        "              .withColumn(\"rank\", dense_rank().over(wcat))\n"
      ],
      "metadata": {
        "id": "qHhKeLT8V89q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_per_cat = prod_rank.filter(\"rank = 1\")"
      ],
      "metadata": {
        "id": "kaGw-WqIV9D8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top3_cities = city_rank.filter(col(\"rank\") <= 3)"
      ],
      "metadata": {
        "id": "8tR0teumWDRO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 7 — BROADCAST JOIN"
      ],
      "metadata": {
        "id": "K4fdl8rgWFIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [\n",
        " (\"Delhi\",\"North\"), (\"Mumbai\",\"West\"), (\"Bangalore\",\"South\"),\n",
        " (\"Hyderabad\",\"South\"), (\"Pune\",\"West\"), (\"Chennai\",\"South\"), (\"Kolkata\",\"East\")\n",
        "]\n",
        "\n",
        "region_df = spark.createDataFrame(data, [\"city\",\"region\"])\n"
      ],
      "metadata": {
        "id": "lhrY_t3SWG9U"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "df_joined = df.join(broadcast(region_df), df.city_clean == region_df.city, \"left\")\n"
      ],
      "metadata": {
        "id": "nciZq3ZdWMzv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cfZTA3uWQSp",
        "outputId": "5085c4e3-da82-4a5f-f22d-bde1261b2ee3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "Join LeftOuter, (city_clean#74 = city#616)\n",
            ":- Deduplicate [order_id#17]\n",
            ":  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, cast(coalesce(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            ":     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#123) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#124]\n",
            ":        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#122) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#123]\n",
            ":           +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#122]\n",
            ":              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#81) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#82]\n",
            ":                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#80) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#81]\n",
            ":                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#80]\n",
            ":                       +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79]\n",
            ":                          +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 ELSE cast(null as string) END AS amount_clean#78]\n",
            ":                             +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            ":                                +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, initcap(product#73) AS product_clean#76]\n",
            ":                                   +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, initcap(category#72) AS category_clean#75]\n",
            ":                                      +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74]\n",
            ":                                         +- Project [order_id#17, customer_id#18, city#71, category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            ":                                            +- Project [order_id#17, customer_id#18, city#71, trim(category#20, None) AS category#72, product#21, amount#22, order_date#23, status#24]\n",
            ":                                               +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, category#20, product#21, amount#22, order_date#23, status#24]\n",
            ":                                                  +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#616, region#617], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, city_clean: string, category_clean: string, product_clean: string, amount_clean: string, amount_int: int, order_date_clean: date, city: string, region: string\n",
            "Join LeftOuter, (city_clean#74 = city#616)\n",
            ":- Deduplicate [order_id#17]\n",
            ":  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, cast(coalesce(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            ":     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#123) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#124]\n",
            ":        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#122) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#123]\n",
            ":           +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#122]\n",
            ":              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#81) THEN to_date(order_date#23, Some(yyyy/MM/dd), Some(Etc/UTC), true) END AS order_date_clean#82]\n",
            ":                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, CASE WHEN isnull(order_date_clean#80) THEN to_date(order_date#23, Some(dd/MM/yyyy), Some(Etc/UTC), true) END AS order_date_clean#81]\n",
            ":                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, amount_int#79, to_date(order_date#23, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS order_date_clean#80]\n",
            ":                       +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79]\n",
            ":                          +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 ELSE cast(null as string) END AS amount_clean#78]\n",
            ":                             +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            ":                                +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, initcap(product#73) AS product_clean#76]\n",
            ":                                   +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, initcap(category#72) AS category_clean#75]\n",
            ":                                      +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74]\n",
            ":                                         +- Project [order_id#17, customer_id#18, city#71, category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            ":                                            +- Project [order_id#17, customer_id#18, city#71, trim(category#20, None) AS category#72, product#21, amount#22, order_date#23, status#24]\n",
            ":                                               +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, category#20, product#21, amount#22, order_date#23, status#24]\n",
            ":                                                  +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#616, region#617], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Join LeftOuter, (city_clean#633 = city#616), rightHint=(strategy=broadcast)\n",
            ":- Aggregate [order_id#17], [order_id#17, first(customer_id#18, false) AS customer_id#619, first(city#71, false) AS city#621, first(category#72, false) AS category#623, first(product#73, false) AS product#625, first(amount#22, false) AS amount#627, first(order_date#23, false) AS order_date#629, first(status#24, false) AS status#631, first(city_clean#74, false) AS city_clean#633, first(category_clean#75, false) AS category_clean#635, first(product_clean#76, false) AS product_clean#637, first(amount_clean#78, false) AS amount_clean#639, first(amount_int#79, false) AS amount_int#641, first(order_date_clean#127, false) AS order_date_clean#643]\n",
            ":  +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79, cast(coalesce(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            ":     +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 END AS amount_clean#78]\n",
            ":        +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74, initcap(category#72) AS category_clean#75, initcap(product#73) AS product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            ":           +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, trim(category#20, None) AS category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            ":              +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- Filter isnotnull(city#616)\n",
            "   +- LogicalRDD [city#616, region#617], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [city_clean#633], [city#616], LeftOuter, BuildRight, false\n",
            "   :- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#71, false), first(category#72, false), first(product#73, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#74, false), first(category_clean#75, false), first(product_clean#76, false), first(amount_clean#78, false), first(amount_int#79, false), first(order_date_clean#127, false)], output=[order_id#17, customer_id#619, city#621, category#623, product#625, amount#627, order_date#629, status#631, city_clean#633, category_clean#635, product_clean#637, amount_clean#639, amount_int#641, order_date_clean#643])\n",
            "   :  +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "   :     +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=679]\n",
            "   :        +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#71, false), partial_first(category#72, false), partial_first(product#73, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#74, false), partial_first(category_clean#75, false), partial_first(product_clean#76, false), partial_first(amount_clean#78, false), partial_first(amount_int#79, false), partial_first(order_date_clean#127, false)], output=[order_id#17, first#670, valueSet#671, first#672, valueSet#673, first#674, valueSet#675, first#676, valueSet#677, first#678, valueSet#679, first#680, valueSet#681, first#682, valueSet#683, first#684, valueSet#685, first#686, valueSet#687, first#688, valueSet#689, first#690, valueSet#691, first#692, valueSet#693, ... 2 more fields])\n",
            "   :           +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "   :              +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, amount_clean#78, cast(amount_clean#78 as int) AS amount_int#79, cast(coalesce(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) as date) AS order_date_clean#127]\n",
            "   :                 +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, city_clean#74, category_clean#75, product_clean#76, CASE WHEN RLIKE(amount_clean#77, ^[0-9]+$) THEN amount_clean#77 END AS amount_clean#78]\n",
            "   :                    +- Project [order_id#17, customer_id#18, city#71, category#72, product#73, amount#22, order_date#23, status#24, initcap(city#71) AS city_clean#74, initcap(category#72) AS category_clean#75, initcap(product#73) AS product_clean#76, regexp_replace(amount#22, ,, , 1) AS amount_clean#77]\n",
            "   :                       +- Project [order_id#17, customer_id#18, trim(city#19, None) AS city#71, trim(category#20, None) AS category#72, trim(product#21, None) AS product#73, amount#22, order_date#23, status#24]\n",
            "   :                          +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=683]\n",
            "      +- Filter isnotnull(city#616)\n",
            "         +- Scan ExistingRDD[city#616,region#617]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why broadcast join is efficient?\n",
        "\n",
        "* Lookup table is small\n",
        "* Sent to every executor once\n",
        "* Avoids shuffling large orders dataset\n",
        "* Fastest join method in Spark for small reference tables"
      ],
      "metadata": {
        "id": "1XxfaS3AWYQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 8 — UDF"
      ],
      "metadata": {
        "id": "nJCl3LCVWp7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def classify(v):\n",
        "    if v is None: return \"Low\"\n",
        "    if v >= 80000: return \"High\"\n",
        "    if v >= 40000: return \"Medium\"\n",
        "    return \"Low\"\n",
        "\n",
        "classify_udf = udf(classify, StringType())\n",
        "\n",
        "df = df.withColumn(\"order_value_category\", classify_udf(col(\"amount_int\")))\n"
      ],
      "metadata": {
        "id": "bFJktnmQWUA3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"order_value_category\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL_8nRzwWvRo",
        "outputId": "1a6639c1-5da7-4410-c538-8f95e990e15a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|order_value_category| count|\n",
            "+--------------------+------+\n",
            "|                High| 29371|\n",
            "|                 Low|153451|\n",
            "|              Medium|117178|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 9 — RDD"
      ],
      "metadata": {
        "id": "CBO2gNzFWyFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df.rdd"
      ],
      "metadata": {
        "id": "8RyZi6IhW1iI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rev = rdd.map(lambda r: r[\"amount_int\"] or 0).reduce(lambda x,y: x+y)"
      ],
      "metadata": {
        "id": "kbZ39DuXW8RD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "city_orders = rdd.map(lambda r: (r[\"city_clean\"], 1)).reduceByKey(lambda a,b: a+b)\n",
        "\n"
      ],
      "metadata": {
        "id": "G5vQoYqBXGYr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why DataFrames are better than RDDs?\n",
        "\n",
        "* Catalyst optimizer\n",
        "* Tungsten execution engine\n",
        "* Predicate pushdown\n",
        "* Columnar optimization\n",
        "* Much faster\n",
        "* Safer, less code\n",
        "* More memory-efficient"
      ],
      "metadata": {
        "id": "u3W6xeX1XOcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 10 — CACHING"
      ],
      "metadata": {
        "id": "DNCzJEUKXhfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv8GI5jGXj9J",
        "outputId": "79009ec2-64ea-48f4-e9ad-11e06139deeb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, city_clean: string, category_clean: string, product_clean: string, amount_clean: string, amount_int: int, order_date_clean: date, order_value_category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.groupBy(\"city_clean\").sum(\"amount_int\").show()\n",
        "df.groupBy(\"category_clean\").avg(\"amount_int\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZJarec9Xm9I",
        "outputId": "e72c9775-5588-4b4d-da78-12c2e4f43d1a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------+\n",
            "|city_clean|sum(amount_int)|\n",
            "+----------+---------------+\n",
            "| Bangalore|     1713862477|\n",
            "|   Chennai|     1714214871|\n",
            "|    Mumbai|     1712702171|\n",
            "|   Kolkata|     1708896076|\n",
            "|      Pune|     1733418439|\n",
            "|     Delhi|     1719458186|\n",
            "| Hyderabad|     1731794799|\n",
            "+----------+---------------+\n",
            "\n",
            "+--------------+------------------+\n",
            "|category_clean|   avg(amount_int)|\n",
            "+--------------+------------------+\n",
            "|          Home|  43861.3925077651|\n",
            "|       Fashion| 43661.82383875079|\n",
            "|       Grocery|43865.714036058605|\n",
            "|   Electronics| 43759.72683138903|\n",
            "+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apSRTknJXnER",
        "outputId": "bfa9f947-d6ba-4bc6-f915-a2842b8f4845"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, city_clean: string, category_clean: string, product_clean: string, amount_clean: string, amount_int: int, order_date_clean: date, order_value_category: string]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 11 — STORAGE FORMATS"
      ],
      "metadata": {
        "id": "2Q5if0axX9BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"city_clean\").parquet(\"clean_orders_parquet\")"
      ],
      "metadata": {
        "id": "v1F2PKg3X_lm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_city.write.orc(\"rev_city_orc\")"
      ],
      "metadata": {
        "id": "-DaWTGpgYGqU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p = spark.read.parquet(\"clean_orders_parquet\")\n",
        "o = spark.read.orc(\"rev_city_orc\")\n",
        "p.printSchema()\n",
        "o.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKPX7vzbYMFO",
        "outputId": "25b5c3ce-6b7b-48fc-cd05-7093955cf701"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- category_clean: string (nullable = true)\n",
            " |-- product_clean: string (nullable = true)\n",
            " |-- amount_clean: string (nullable = true)\n",
            " |-- amount_int: integer (nullable = true)\n",
            " |-- order_date_clean: date (nullable = true)\n",
            " |-- order_value_category: string (nullable = true)\n",
            " |-- city_clean: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- city_clean: string (nullable = true)\n",
            " |-- sum(amount_int): long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Parquet/ORC vs CSV\n",
        "\n",
        "* Columnar\n",
        "* Compressed\n",
        "* Much smaller size\n",
        "* Predicate pushdown\n",
        "* Way faster for analytics\n",
        "\n"
      ],
      "metadata": {
        "id": "9Urab2YgYQLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 12 — DEBUGGING"
      ],
      "metadata": {
        "id": "KQhlKN-DYXMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why does this fail?"
      ],
      "metadata": {
        "id": "-zOTWY7WYj4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(df.amount > 50000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "yU_a62SMYZgY",
        "outputId": "d4847966-d502-4d98-86de-fb6c03d29666"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2026-01-15 11:19:35.152\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'invalid' of the type \\\"STRING\\\" cannot be cast to \\\"BIGINT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 1 in cell [53]\", \"line\": \"\", \"fragment\": \"__gt__\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o512.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'invalid' of the type \\\"STRING\\\" cannot be cast to \\\"BIGINT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"__gt__\\\" was called from\\nline 1 in cell [53]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toLongExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat scala.collection.Iterator$GroupedIterator.fulfill(Iterator.scala:242)\\n\\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\\n\\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\\n\\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.writeNextInputToStream(PythonUDFRunner.scala:69)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\\n\\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\\n\\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\\n\\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\\n\\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:98)\\n\\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\\n\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\\n\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\\n\\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\\n\\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NumberFormatException",
          "evalue": "[CAST_INVALID_INPUT] The value 'invalid' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__gt__\" was called from\nline 1 in cell [53]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-568713430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value 'invalid' of the type \"STRING\" cannot be cast to \"BIGINT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"__gt__\" was called from\nline 1 in cell [53]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because:\n",
        "\n",
        "* .show() returns None\n",
        "* You assign None back to df\n",
        "* df is no longer a DataFrame\n",
        "\n",
        "Correct usage:"
      ],
      "metadata": {
        "id": "HeGIUCj_YhMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import trim, col, initcap, regexp_replace, when, coalesce, try_to_timestamp, lit\n",
        "\n",
        "# Re-apply cleaning steps to ensure df is in a consistent state with amount_int correctly processed\n",
        "df = df_raw.withColumn(\"city\", trim(col(\"city\"))) \\\n",
        "           .withColumn(\"category\", trim(col(\"category\"))) \\\n",
        "           .withColumn(\"product\", trim(col(\"product\")))\n",
        "\n",
        "df = df.withColumn(\"city_clean\", initcap(col(\"city\"))) \\\n",
        "       .withColumn(\"category_clean\", initcap(col(\"category\"))) \\\n",
        "       .withColumn(\"product_clean\", initcap(col(\"product\")))\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"amount_clean\",\n",
        "    regexp_replace(\"amount\", \",\", \"\")\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"amount_clean\",\n",
        "    when(col(\"amount_clean\").rlike(\"^[0-9]+$\"), col(\"amount_clean\")).otherwise(None)\n",
        ")\n",
        "\n",
        "df = df.withColumn(\"amount_int\", col(\"amount_clean\").cast(\"int\"))\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"order_date_clean\",\n",
        "    coalesce(\n",
        "        try_to_timestamp(col(\"order_date\"), lit(\"yyyy-MM-dd\")),\n",
        "        try_to_timestamp(col(\"order_date\"), lit(\"dd/MM/yyyy\")),\n",
        "        try_to_timestamp(col(\"order_date\"), lit(\"yyyy/MM/dd\"))\n",
        "    ).cast(\"date\")\n",
        ")\n",
        "\n",
        "df = df.dropDuplicates([\"order_id\"])\n",
        "\n",
        "# Now perform the filter operation\n",
        "df = df.filter(df.amount_int > 50000)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjTHof-4YfkE",
        "outputId": "b1594c3a-4fbd-467e-da04-106db2b3c026"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+----------+--------------+-------------+------------+----------+----------------+\n",
            "|   order_id|customer_id|     city|   category|    product|amount|order_date|   status|city_clean|category_clean|product_clean|amount_clean|amount_int|order_date_clean|\n",
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+----------+--------------+-------------+------------+----------+----------------+\n",
            "|ORD00000010|    C000010|Bangalore|    Grocery|      Sugar| 66576|2024-01-11|Completed| Bangalore|       Grocery|        Sugar|       66576|     66576|      2024-01-11|\n",
            "|ORD00000011|    C000011|  Kolkata|Electronics|     Tablet| 50318|12/01/2024|Completed|   Kolkata|   Electronics|       Tablet|       50318|     50318|      2024-01-12|\n",
            "|ORD00000012|    C000012|Bangalore|    Grocery|      Sugar| 84768|2024-01-13|Completed| Bangalore|       Grocery|        Sugar|       84768|     84768|      2024-01-13|\n",
            "|ORD00000014|    C000014|   Mumbai|Electronics|     Tablet| 79469|2024-01-15|Completed|    Mumbai|   Electronics|       Tablet|       79469|     79469|      2024-01-15|\n",
            "|ORD00000015|    C000015|     Pune|Electronics|     Mobile| 81018|2024-01-16|Completed|      Pune|   Electronics|       Mobile|       81018|     81018|      2024-01-16|\n",
            "|ORD00000017|    C000017|bangalore|    Grocery|        Oil| 69582|2024-01-18|Completed| Bangalore|       Grocery|          Oil|       69582|     69582|      2024-01-18|\n",
            "|ORD00000025|    C000025|Bangalore|       Home|AirPurifier| 58248|2024-01-26|Completed| Bangalore|          Home|  Airpurifier|       58248|     58248|      2024-01-26|\n",
            "|ORD00000028|    C000028|   Mumbai|    Grocery|      Sugar| 70675|2024-01-29|Completed|    Mumbai|       Grocery|        Sugar|       70675|     70675|      2024-01-29|\n",
            "|ORD00000030|    C000030|     Pune|       Home|AirPurifier| 52112|2024-01-31|Completed|      Pune|          Home|  Airpurifier|       52112|     52112|      2024-01-31|\n",
            "|ORD00000031|    C000031|  Chennai|    grocery|        Oil| 51151|2024-02-01|Completed|   Chennai|       Grocery|          Oil|       51151|     51151|      2024-02-01|\n",
            "|ORD00000032|    C000032|  Chennai|       Home|     Vacuum| 75797|2024-02-02|Completed|   Chennai|          Home|       Vacuum|       75797|     75797|      2024-02-02|\n",
            "|ORD00000039|    C000039|     Pune|    Grocery|        Oil| 73088|2024/02/09|Completed|      Pune|       Grocery|          Oil|       73088|     73088|      2024-02-09|\n",
            "|ORD00000040|    C000040|    Delhi|    Grocery|        Oil| 81145|2024-02-10|Cancelled|     Delhi|       Grocery|          Oil|       81145|     81145|      2024-02-10|\n",
            "|ORD00000042|    C000042|    Delhi|       Home|     Vacuum| 83855|2024-02-12|Completed|     Delhi|          Home|       Vacuum|       83855|     83855|      2024-02-12|\n",
            "|ORD00000043|    C000043|Hyderabad|       Home|AirPurifier| 82061|2024-02-13|Completed| Hyderabad|          Home|  Airpurifier|       82061|     82061|      2024-02-13|\n",
            "|ORD00000044|    C000044|    Delhi|    Grocery|      Sugar| 64841|14/02/2024|Completed|     Delhi|       Grocery|        Sugar|       64841|     64841|      2024-02-14|\n",
            "|ORD00000048|    C000048|  Kolkata|    Fashion|      Jeans| 51000|2024-02-18|Completed|   Kolkata|       Fashion|        Jeans|       51000|     51000|      2024-02-18|\n",
            "|ORD00000061|    C000061|Hyderabad|       Home|AirPurifier| 54290|2024-01-02|Completed| Hyderabad|          Home|  Airpurifier|       54290|     54290|      2024-01-02|\n",
            "|ORD00000066|    C000066|  Chennai|Electronics|     Mobile| 78373|07/01/2024|Completed|   Chennai|   Electronics|       Mobile|       78373|     78373|      2024-01-07|\n",
            "|ORD00000067|    C000067|  Chennai|       Home|AirPurifier| 88927|2024-01-08|Completed|   Chennai|          Home|  Airpurifier|       88927|     88927|      2024-01-08|\n",
            "+-----------+-----------+---------+-----------+-----------+------+----------+---------+----------+--------------+-------------+------------+----------+----------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 13 — FINAL VALIDATION"
      ],
      "metadata": {
        "id": "GH-WSLIRZ7mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NczQLBelZ_se",
        "outputId": "78590755-10c5-4b8c-aa8f-635f495f21d4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- city_clean: string (nullable = true)\n",
            " |-- category_clean: string (nullable = true)\n",
            " |-- product_clean: string (nullable = true)\n",
            " |-- amount_clean: string (nullable = true)\n",
            " |-- amount_int: integer (nullable = true)\n",
            " |-- order_date_clean: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\"city_clean IS NULL or category_clean IS NULL or product_clean IS NULL\").count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "O-By8lJmaErC",
        "outputId": "56279124-9524-4b7d-be90-bf85e3cee7d3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.count of DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, city_clean: string, category_clean: string, product_clean: string, amount_clean: string, amount_int: int, order_date_clean: date]>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.classic.dataframe.DataFrame.count</b><br/>def count() -&gt; int</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py</a>Returns the number of rows in this :class:`DataFrame`.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "int\n",
              "    Number of rows.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; df = spark.createDataFrame(\n",
              "...     [(14, &quot;Tom&quot;), (23, &quot;Alice&quot;), (16, &quot;Bob&quot;)], [&quot;age&quot;, &quot;name&quot;])\n",
              "\n",
              "Return the number of rows in the :class:`DataFrame`.\n",
              "\n",
              "&gt;&gt;&gt; df.count()\n",
              "3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 438);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Documentation Summary\n",
        "\n",
        "Cleaning Strategy\n",
        "\n",
        "* Keep original columns\n",
        "* Trim + proper case\n",
        "* Clean amounts safely\n",
        "* Parse multi-format dates\n",
        "* Handle invalid rows gently\n",
        "\n",
        "Performance Strategy\n",
        "\n",
        "* Partitioning\n",
        "* Broadcast join\n",
        "* Cache only when needed\n",
        "* Avoid shuffles\n",
        "* File formats Parquet/ORC\n",
        "\n",
        "Debugging Learnings\n",
        "\n",
        "* .show() destroys DataFrames\n",
        "* Always separate transformations and actions\n",
        "* Schema inference can corrupt data\n",
        "\n"
      ],
      "metadata": {
        "id": "lLWM7CNeaJoS"
      }
    }
  ]
}