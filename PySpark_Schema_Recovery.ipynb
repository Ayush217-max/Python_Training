{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNsHM/HJnV33xovDDJUmuOz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAXu-yyi-_yP","executionInfo":{"status":"ok","timestamp":1766040653675,"user_tz":-330,"elapsed":3508,"user":{"displayName":"Ayush","userId":"02144861623101027601"}},"outputId":"7a996e7d-02a0-47fa-abaf-0b26c38619fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-----+----------+---------+----------+----------+\n","|user_id| name|   age_raw|     city|salary_raw|  age_trim|\n","+-------+-----+----------+---------+----------+----------+\n","|   U001| Amit|        29|Hyderabad|     50000|        29|\n","|   U002| Neha|Thirty Two|    Delhi|     62000|Thirty Two|\n","|   U003| Ravi|      NULL|Bangalore|       45k|      NULL|\n","|   U004|Pooja|        28|   Mumbai|     58000|        28|\n","|   U005| NULL|        31|  Chennai|          |        31|\n","+-------+-----+----------+---------+----------+----------+\n","\n","+-------+-----+----------+---------+----------+----------+-------+\n","|user_id| name|   age_raw|     city|salary_raw|  age_trim|age_int|\n","+-------+-----+----------+---------+----------+----------+-------+\n","|   U001| Amit|        29|Hyderabad|     50000|        29|     29|\n","|   U002| Neha|Thirty Two|    Delhi|     62000|Thirty Two|   NULL|\n","|   U003| Ravi|      NULL|Bangalore|       45k|      NULL|   NULL|\n","|   U004|Pooja|        28|   Mumbai|     58000|        28|     28|\n","|   U005| NULL|        31|  Chennai|          |        31|     31|\n","+-------+-----+----------+---------+----------+----------+-------+\n","\n","+-------+-----+----------+---------+----------+----------+-------+----------+\n","|user_id| name|   age_raw|     city|salary_raw|  age_trim|age_int|salary_int|\n","+-------+-----+----------+---------+----------+----------+-------+----------+\n","|   U001| Amit|        29|Hyderabad|     50000|        29|     29|     50000|\n","|   U002| Neha|Thirty Two|    Delhi|     62000|Thirty Two|   NULL|     62000|\n","|   U003| Ravi|      NULL|Bangalore|       45k|      NULL|   NULL|     45000|\n","|   U004|Pooja|        28|   Mumbai|     58000|        28|     28|     58000|\n","|   U005| NULL|        31|  Chennai|          |        31|     31|      NULL|\n","+-------+-----+----------+---------+----------+----------+-------+----------+\n","\n","+-------+-----+----------+---------+----------+----------+-------+----------+----------+\n","|user_id| name|   age_raw|     city|salary_raw|  age_trim|age_int|salary_int|name_clean|\n","+-------+-----+----------+---------+----------+----------+-------+----------+----------+\n","|   U001| Amit|        29|Hyderabad|     50000|        29|     29|     50000|      Amit|\n","|   U002| Neha|Thirty Two|    Delhi|     62000|Thirty Two|   NULL|     62000|      Neha|\n","|   U003| Ravi|      NULL|Bangalore|       45k|      NULL|   NULL|     45000|      Ravi|\n","|   U004|Pooja|        28|   Mumbai|     58000|        28|     28|     58000|     Pooja|\n","|   U005| NULL|        31|  Chennai|          |        31|     31|      NULL|   UNKNOWN|\n","+-------+-----+----------+---------+----------+----------+-------+----------+----------+\n","\n","+-------+-----+-------+---------+----------+--------+-------+----------+----------+\n","|user_id| name|age_raw|     city|salary_raw|age_trim|age_int|salary_int|name_clean|\n","+-------+-----+-------+---------+----------+--------+-------+----------+----------+\n","|   U001| Amit|     29|Hyderabad|     50000|      29|     29|     50000|      Amit|\n","|   U004|Pooja|     28|   Mumbai|     58000|      28|     28|     58000|     Pooja|\n","|   U005| NULL|     31|  Chennai|          |      31|     31|      NULL|   UNKNOWN|\n","+-------+-----+-------+---------+----------+--------+-------+----------+----------+\n","\n","+-------+----+----------+---------+----------+----------+-------+\n","|user_id|name|age_raw   |city     |salary_raw|age_trim  |age_int|\n","+-------+----+----------+---------+----------+----------+-------+\n","|U002   |Neha|Thirty Two|Delhi    |62000     |Thirty Two|NULL   |\n","|U003   |Ravi|NULL      |Bangalore|45k       |NULL      |NULL   |\n","+-------+----+----------+---------+----------+----------+-------+\n","\n","+-------+-------+---+---------+------+\n","|user_id|   name|age|     city|salary|\n","+-------+-------+---+---------+------+\n","|   U001|   Amit| 29|Hyderabad| 50000|\n","|   U004|  Pooja| 28|   Mumbai| 58000|\n","|   U005|UNKNOWN| 31|  Chennai|  NULL|\n","+-------+-------+---+---------+------+\n","\n"]}],"source":["\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n","from pyspark.sql import functions as F\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","raw_users = [\n","    (\"U001\",\"Amit\",\"29\",\"Hyderabad\",\"50000\"),\n","    (\"U002\",\"Neha\",\"Thirty Two\",\"Delhi\",\"62000\"),\n","    (\"U003\",\"Ravi\",None,\"Bangalore\",\"45k\"),\n","    (\"U004\",\"Pooja\",\"28\",\"Mumbai\",58000),\n","    (\"U005\",None,\"31\",\"Chennai\",\"\")\n","]\n","user_schema = StructType([\n","    StructField(\"user_id\", StringType(), False),\n","    StructField(\"name\",    StringType(), True),\n","    StructField(\"age_raw\", StringType(), True),\n","    StructField(\"city\",    StringType(), True),\n","    StructField(\"salary_raw\", StringType(), True)\n","])\n","users_df = spark.createDataFrame(raw_users, schema=user_schema)\n","\n","users_df = users_df.withColumn(\"age_trim\", F.trim(F.col(\"age_raw\")))\n","users_df.show()\n","users_df = users_df.withColumn(\"age_int\",\n","    F.when(F.col(\"age_trim\").rlike(r\"^[0-9]+$\"), F.col(\"age_trim\").cast(IntegerType()))\n","     .otherwise(F.lit(None).cast(IntegerType()))\n",")\n","users_df.show()\n","\n","conversion_fail_df = users_df.filter(F.col(\"age_int\").isNull())\n","\n","# 4) Convert age to integer safely (already done in age_int)\n","# 5) Normalize salary into integer (handle 'k', empty, None)\n","def normalize_salary(col):\n","    return F.when(F.col(col).isNull() | (F.trim(F.col(col)) == \"\"), F.lit(None).cast(IntegerType())) \\\n","            .when(F.lower(F.trim(F.col(col))).rlike(r\"^[0-9]+k$\"),\n","                  (F.regexp_extract(F.lower(F.trim(F.col(col))), r\"^([0-9]+)k$\", 1).cast(IntegerType()) * F.lit(1000))) \\\n","            .when(F.trim(F.col(col)).rlike(r\"^[0-9]+$\"),\n","                  F.trim(F.col(col)).cast(IntegerType())) \\\n","            .otherwise(F.lit(None).cast(IntegerType()))\n","\n","users_df = users_df.withColumn(\"salary_int\", normalize_salary(\"salary_raw\"))\n","users_df.show()\n","\n","users_df = users_df.withColumn(\"name_clean\",\n","    F.when(F.col(\"name\").isNull() | (F.trim(F.col(\"name\")) == \"\"), F.lit(\"UNKNOWN\"))\n","     .otherwise(F.col(\"name\"))\n",")\n","users_df.show()\n","\n","\n","clean_users_df = users_df.filter(F.col(\"age_int\").isNotNull())\n","clean_users_df.show()\n","\n","\n","final_users_df = clean_users_df.select(\n","    \"user_id\",\n","    F.col(\"name_clean\").alias(\"name\"),\n","    F.col(\"age_int\").alias(\"age\"),\n","    \"city\",\n","    F.col(\"salary_int\").alias(\"salary\")\n",")\n","\n","\n","conversion_fail_df.show(truncate=False)\n","final_users_df.show()\n","\n","\n"]},{"cell_type":"code","source":["\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n","from pyspark.sql import functions as F\n","\n","raw_orders = [\n","    (\"O001\",\"U001\",\"Laptop,Mobile,Tablet\",75000),\n","    (\"O002\",\"U002\",[\"Mobile\",\"Tablet\"],32000),\n","    (\"O003\",\"U003\",\"Laptop\",72000),\n","    (\"O004\",\"U004\",None,25000),\n","    (\"O005\",\"U005\",\"Laptop\\nMobile\",68000)\n","]\n","\n","# 1) Schema with ArrayType for items (we'll ingest as string and array flexibly)\n","orders_schema = StructType([\n","    StructField(\"order_id\", StringType(), False),\n","    StructField(\"user_id\",  StringType(), False),\n","    StructField(\"items_raw\", StringType(), True),   # we will harmonize to arrays\n","    StructField(\"amount\",   IntegerType(), True)\n","])\n","\n","# Create DF but we need to coerce list entries to strings to fit schema uniformly\n","# So we transform raw_orders: if items is list -> join with comma; if None -> None; else str\n","def to_uniform_row(row):\n","    order_id, user_id, items, amount = row\n","    if items is None:\n","        items_str = None\n","    elif isinstance(items, list):\n","        items_str = \",\".join(items)\n","    else:\n","        items_str = str(items)\n","    return (order_id, user_id, items_str, int(amount) if amount is not None else None)\n","\n","uniform_orders = list(map(to_uniform_row, raw_orders))\n","print(uniform_orders)\n","\n","orders_df = spark.createDataFrame(uniform_orders, schema=orders_schema)\n","\n","\n","split_pattern = r\"[,\\n]+\"\n","\n","orders_df = orders_df.withColumn(\n","    \"items_array\",\n","    F.when(F.col(\"items_raw\").isNull(), F.array())  # 4) replace null items with empty arrays\n","     .otherwise(\n","         F.filter(\n","             F.transform(\n","                 F.split(F.col(\"items_raw\"), split_pattern),\n","                 lambda x: F.trim(x)\n","             ),\n","             lambda x: x != \"\"\n","         )\n","     )\n",")\n","orders_df.show()\n","\n","# 5) Explode items into one row per item\n","exploded_df = orders_df.select(\"order_id\", \"user_id\", \"amount\", F.explode(\"items_array\").alias(\"item\"))\n","\n","# 6) Count frequency of each item\n","item_freq_df = exploded_df.groupBy(\"item\").agg(F.count(\"*\").alias(\"freq\")).orderBy(F.desc(\"freq\"))\n","\n","# 7) Identify orders with more than 2 items\n","orders_with_counts = orders_df.withColumn(\"item_count\", F.size(\"items_array\"))\n","orders_gt2_df = orders_with_counts.filter(F.col(\"item_count\") > 2)\n","\n","print(\"Orders normalized:\")\n","orders_df.show(truncate=False)\n","\n","print(\"Exploded items:\")\n","exploded_df.show(truncate=False)\n","\n","print(\"Item frequencies:\")\n","item_freq_df.show(truncate=False)\n","\n","print(\"Orders with > 2 items:\")\n","orders_gt2_df.select(\"order_id\", \"user_id\", \"item_count\").show(truncate=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTC-I3UQUOYD","executionInfo":{"status":"ok","timestamp":1766042811559,"user_tz":-330,"elapsed":2205,"user":{"displayName":"Ayush","userId":"02144861623101027601"}},"outputId":"3ce4aef8-ac25-4a1d-f537-cf492ba2d4aa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[('O001', 'U001', 'Laptop,Mobile,Tablet', 75000), ('O002', 'U002', 'Mobile,Tablet', 32000), ('O003', 'U003', 'Laptop', 72000), ('O004', 'U004', None, 25000), ('O005', 'U005', 'Laptop\\nMobile', 68000)]\n","+--------+-------+--------------------+------+--------------------+\n","|order_id|user_id|           items_raw|amount|         items_array|\n","+--------+-------+--------------------+------+--------------------+\n","|    O001|   U001|Laptop,Mobile,Tablet| 75000|[Laptop, Mobile, ...|\n","|    O002|   U002|       Mobile,Tablet| 32000|    [Mobile, Tablet]|\n","|    O003|   U003|              Laptop| 72000|            [Laptop]|\n","|    O004|   U004|                NULL| 25000|                  []|\n","|    O005|   U005|      Laptop\\nMobile| 68000|    [Laptop, Mobile]|\n","+--------+-------+--------------------+------+--------------------+\n","\n","Orders normalized:\n","+--------+-------+--------------------+------+------------------------+\n","|order_id|user_id|items_raw           |amount|items_array             |\n","+--------+-------+--------------------+------+------------------------+\n","|O001    |U001   |Laptop,Mobile,Tablet|75000 |[Laptop, Mobile, Tablet]|\n","|O002    |U002   |Mobile,Tablet       |32000 |[Mobile, Tablet]        |\n","|O003    |U003   |Laptop              |72000 |[Laptop]                |\n","|O004    |U004   |NULL                |25000 |[]                      |\n","|O005    |U005   |Laptop\\nMobile      |68000 |[Laptop, Mobile]        |\n","+--------+-------+--------------------+------+------------------------+\n","\n","Exploded items:\n","+--------+-------+------+------+\n","|order_id|user_id|amount|item  |\n","+--------+-------+------+------+\n","|O001    |U001   |75000 |Laptop|\n","|O001    |U001   |75000 |Mobile|\n","|O001    |U001   |75000 |Tablet|\n","|O002    |U002   |32000 |Mobile|\n","|O002    |U002   |32000 |Tablet|\n","|O003    |U003   |72000 |Laptop|\n","|O005    |U005   |68000 |Laptop|\n","|O005    |U005   |68000 |Mobile|\n","+--------+-------+------+------+\n","\n","Item frequencies:\n","+------+----+\n","|item  |freq|\n","+------+----+\n","|Laptop|3   |\n","|Mobile|3   |\n","|Tablet|2   |\n","+------+----+\n","\n","Orders with > 2 items:\n","+--------+-------+----------+\n","|order_id|user_id|item_count|\n","+--------+-------+----------+\n","|O001    |U001   |3         |\n","+--------+-------+----------+\n","\n"]}]},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n","from pyspark.sql import functions as F\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# -----------------------------\n","# 0) Raw input (as provided)\n","# -----------------------------\n","raw_users = [\n","    (\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n","    (\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n","    (\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n","    (\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n","    (\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")\n","]\n","\n","# -------------------------------------------------------\n","# 1) Explicit schema (store skills as STRING initially)\n","# -------------------------------------------------------\n","users_schema = StructType([\n","    StructField(\"user_id\",   StringType(), False),\n","    StructField(\"name_raw\",  StringType(), True),\n","    StructField(\"age_raw\",   StringType(), True),\n","    StructField(\"city\",      StringType(), True),\n","    StructField(\"skills_raw\",StringType(), True)   # normalize to string for robust parsing\n","])\n","\n","# We will convert any Python list in raw_users to a comma-separated string\n","def normalize_row(row):\n","    user_id, name, age, city, skills = row\n","    # skills: list -> \"a,b\", None -> None, str -> as-is\n","    if isinstance(skills, list):\n","        skills_str = \",\".join(map(str, skills))\n","    else:\n","        skills_str = None if skills is None else str(skills)\n","    # age: ensure string/None\n","    age_str = None if age is None else str(age)\n","    # name: keep as-is (string or empty)\n","    return (str(user_id), None if name is None else str(name), age_str, str(city), skills_str)\n","\n","uniform_rows = list(map(normalize_row, raw_users))\n","\n","users_df = spark.createDataFrame(uniform_rows, schema=users_schema)\n","\n","# -------------------------------------------------------\n","# 2) Normalize age into IntegerType\n","# -------------------------------------------------------\n","# Rule:\n","# - If age_raw is digits only -> cast to int\n","# - If age_raw equals 'thirty' (case-insensitive) -> 30\n","# - Else -> null\n","users_df = users_df.withColumn(\"age_trim\", F.trim(F.col(\"age_raw\")))\n","users_df = users_df.withColumn(\n","    \"age_int\",\n","    F.when(F.col(\"age_trim\").rlike(r\"^[0-9]+$\"), F.col(\"age_trim\").cast(IntegerType()))\n","     .when(F.lower(F.col(\"age_trim\")) == F.lit(\"thirty\"), F.lit(30))\n","     .otherwise(F.lit(None).cast(IntegerType()))\n",")\n","\n","# -------------------------------------------------------\n","# 3) Normalize skills into ArrayType(StringType)\n","# -------------------------------------------------------\n","# We handle formats:\n","# - \"['AI','ML','Cloud']\"  -> strip [ ], strip quotes -> split by comma\n","# - \"AI,Testing\"           -> split by comma\n","# - None                   -> empty array\n","# After split: trim tokens and drop blanks.\n","\n","# Step A: default null -> empty string for string preprocessing\n","skills_str = F.coalesce(F.col(\"skills_raw\"), F.lit(\"\"))\n","\n","# Step B: remove surrounding [ ] if present, and remove single/double quotes\n","skills_no_brackets = F.regexp_replace(skills_str, r\"^\\s*\\[|\\]\\s*$\", \"\")  # strip leading '[' and trailing ']'\n","skills_no_quotes   = F.regexp_replace(skills_no_brackets, r\"[\\\"']\", \"\")  # drop any quotes\n","\n","# Step C: split by comma, then trim each item, then filter out empty items\n","split_items = F.split(skills_no_quotes, r\"\\s*,\\s*\")\n","trimmed_items = F.transform(split_items, lambda x: F.trim(x))\n","nonempty_items = F.filter(trimmed_items, lambda x: x != \"\")\n","\n","# Step D: if original was null -> empty array; otherwise use parsed array\n","users_df = users_df.withColumn(\n","    \"skills\",\n","    F.when(F.col(\"skills_raw\").isNull(), F.array())  # empty array\n","     .otherwise(nonempty_items)\n",")\n","\n","# (Optional) deduplicate skills and standardize case (uncomment if desired)\n","# users_df = users_df.withColumn(\n","#     \"skills\",\n","#     F.array_distinct(F.transform(F.col(\"skills\"), lambda x: F.initcap(x)))  # e.g., \"ml\" -> \"Ml\"\n","# )\n","\n","# -------------------------------------------------------\n","# 4) Handle empty or missing names\n","# -------------------------------------------------------\n","users_df = users_df.withColumn(\n","    \"name\",\n","    F.when(F.col(\"name_raw\").isNull() | (F.trim(F.col(\"name_raw\")) == \"\"), F.lit(\"UNKNOWN\"))\n","     .otherwise(F.col(\"name_raw\"))\n",")\n","\n","# -------------------------------------------------------\n","# 5) Produce the final clean users_df\n","# -------------------------------------------------------\n","final_users_df = users_df.select(\n","    \"user_id\",\n","    \"name\",\n","    F.col(\"age_int\").alias(\"age\"),\n","    \"city\",\n","    \"skills\"\n",")\n","\n","# (Optional) If you want to drop rows where age is unrecoverable:\n","# final_users_df = final_users_df.filter(F.col(\"age\").isNotNull())\n","\n","# Show results\n","print(\"Final cleaned users_df:\")\n","final_users_df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dg47paJB8fB4","executionInfo":{"status":"ok","timestamp":1766052724352,"user_tz":-330,"elapsed":499,"user":{"displayName":"Ayush","userId":"02144861623101027601"}},"outputId":"358ee372-0bfa-41be-dc5e-ac29bf354c6a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Final cleaned users_df:\n"]},{"output_type":"execute_result","data":{"text/plain":["DataFrame[user_id: string, name: string, age: int, city: string, skills: array<string>]"]},"metadata":{},"execution_count":9}]}]}